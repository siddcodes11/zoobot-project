# -*- coding: utf-8 -*-
"""galactic_classification_gz2data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1inzWtkTT4_miJTw2ZadTUinw8AbpHPlD
"""

import logging

logging.basicConfig(level=logging.INFO)

!git clone https://github.com/mwalmsley/zoobot.git

!pip install galaxy-datasets[pytorch]

import os
import sys
zoobot_dir = 'zoobot'
os.chdir(zoobot_dir)
sys.path.append(zoobot_dir)
!git pull

from galaxy_datasets import gz2  # or gz_hubble, gz_candels, ...

catalog, label_cols = gz2(
    root='gz2',
    train=True,
    download=True
)

from galaxy_datasets.pytorch import GZ2
import matplotlib.pyplot as plt

gz2_dataset = GZ2(
    root='gz2',
    train=True,
    download=False
)
image, label = gz2_dataset[20000]
plt.imshow(image)
print(label)

from galaxy_datasets.pytorch.galaxy_datamodule import GalaxyDataModule

datamodule = GalaxyDataModule(
    label_cols=['smooth-or-featured-gz2_smooth'],
    catalog=catalog
    # optional args to specify augmentations
)

datamodule.prepare_data()
datamodule.setup()
for images, labels in datamodule.train_dataloader():
    print(images.shape, labels.shape)
    break

catalog.head()

label_cols

checkpoint_dir = os.path.join( 'data/pretrained_models/pytorch')

!wget --no-check-certificate 'https://dl.dropboxusercontent.com/s/7ixwo59imjfz4ay/effnetb0_greyscale_224px.ckpt?dl=0' -O $checkpoint_dir/checkpoint.ckpt1

pip install pyro-ppl

import pandas as pd
from zoobot.pytorch.training import finetune
from galaxy_datasets.pytorch.galaxy_datamodule import GalaxyDataModule

checkpoint_loc = os.path.join('data/pretrained_models/pytorch/checkpoint.ckpt1') 
save_dir = os.path.join('results/pytorch/finetune/finetune_binary_classification')

label_cols=['smooth-or-featured-gz2_smooth','smooth-or-featured-gz2_featured-or-disk','smooth-or-featured-gz2_artifact','disk-edge-on-gz2_yes','disk-edge-on-gz2_no','has-spiral-arms-gz2_yes','has-spiral-arms-gz2_no','bar-gz2_yes','bar-gz2_no','bulge-size-gz2_dominant','bulge-size-gz2_obvious','bulge-size-gz2_just-noticeable','bulge-size-gz2_no','something-odd-gz2_yes','something-odd-gz2_no','how-rounded-gz2_round','how-rounded-gz2_in-between','how-rounded-gz2_cigar','bulge-shape-gz2_round','bulge-shape-gz2_boxy','bulge-shape-gz2_no-bulge','spiral-winding-gz2_tight','spiral-winding-gz2_medium','spiral-winding-gz2_loose','spiral-arm-count-gz2_1','spiral-arm-count-gz2_2','spiral-arm-count-gz2_3','spiral-arm-count-gz2_4','spiral-arm-count-gz2_more-than-4','spiral-arm-count-gz2_cant-tell']  # name of column in catalog with binary (0 or 1) labels for your classes]
 # To support more complicated labels, Zoobot expects a list of columns. A list with one element works fine.

datamodule = GalaxyDataModule(
  label_cols=label_cols,
  catalog= catalog,
  batch_size=32,
  resize_after_crop=224,  # the size of the images input to the model
  num_workers=2# sets the parallelism for loading data. 2 works well on colab.
)

assert all([os.path.isfile(loc) for loc in catalog['file_loc']])

model = finetune.FinetuneableZoobotClassifier(
  checkpoint_loc= checkpoint_loc,
  num_classes=5,
  n_layers=5 # only updating the head weights. Set 0 for only output layer. Set e.g. 1, 2 to finetune deeper. 
)



import torch
import torch.nn as nn
import pytorch_lightning as pl
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint

class FinetuneableZoobotClassifier(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.loss_fn = nn.CrossEntropyLoss()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=3, stride=1, padding=1)
        self.activation = nn.Sigmoid()
        self.fc = nn.Linear(10*224*224, 30)


    def forward(self, x):
        x = self.conv1(x)
        x = self.activation(x)
        x = torch.flatten(x, start_dim=1)
        x = self.fc(x)
        return x
    
    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.loss_fn(logits, y)
        self.log('train_loss', loss, on_epoch=True, on_step=True)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.loss_fn(logits, y)
        self.log('val_loss', loss, on_epoch=True, on_step=True)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer


model = FinetuneableZoobotClassifier()
early_stop_callback = EarlyStopping(monitor='val_loss', patience=5)
checkpoint_callback = ModelCheckpoint(monitor='val_loss')

trainer = pl.Trainer(
    max_epochs=10,
    gpus=1,
    callbacks=[early_stop_callback, checkpoint_callback]
)

# train the model
trainer.fit(model, datamodule)

# get the best checkpoint path
best_checkpoint = checkpoint_callback.best_model_path

# load the best model checkpoint
best_model = FinetuneableZoobotClassifier.load_from_checkpoint(checkpoint_path=best_checkpoint)



from zoobot.pytorch.predictions import predict_on_catalog

predict_on_catalog.predict(
  catalog,
  best_model,
  label_cols=label_cols,
    n_samples=1,
  save_loc=os.path.join(save_dir, '/kaggle/working/finetuned_predictions.csv'),
)

predictions = pd.read_csv(os.path.join(save_dir, '/kaggle/working/finetuned_predictions.csv'))
predictions.head()

predictions = pd.merge(predictions, catalog[['id_str', 'filename', 'smooth-or-featured-gz2_smooth']])
predictions.head()

top_5_predictions = predictions.sort_values('smooth-or-featured-gz2_smooth', ascending=False)
print(top_5_predictions['id_str'])

bottom_5_predictions = predictions.sort_values('smooth-or-featured-gz2_smooth', ascending=True)
print(bottom_5_predictions['id_str'])

random_predictions = predictions.sample(5)
print(random_predictions['id_str'])
